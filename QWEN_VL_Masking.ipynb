{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73ed2b8-41fb-47d6-b032-7515e74b54de",
      "metadata": {
        "scrolled": true,
        "id": "b73ed2b8-41fb-47d6-b032-7515e74b54de"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# üß† INSTALL + SERVER LAUNCH (STABLE VERSION)\n",
        "# ===============================\n",
        "!pip install -U bitsandbytes transformers==4.57.0 matplotlib opencv-python pandas tqdm accelerate gradio_client qwen_vl_utils hf_transfer autoawq docext shapely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57875ab-bd7c-4ba8-b7db-e34a67e86659",
      "metadata": {
        "id": "a57875ab-bd7c-4ba8-b7db-e34a67e86659"
      },
      "outputs": [],
      "source": [
        "import os, re, json, torch, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig, pipeline\n",
        "from PIL import Image\n",
        "from shapely.geometry import Polygon\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9cbc28-0ec5-4a64-9313-7841848410fe",
      "metadata": {
        "id": "5a9cbc28-0ec5-4a64-9313-7841848410fe"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ‚öôÔ∏è Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "# ===============================\n",
        "IMG_DIR = \"Document06/TEST06_image\"\n",
        "JSON_DIR = \"Document06/TEST06_json\"\n",
        "OUT_DIR = \"vlm_outputs6\"\n",
        "MODEL_NAME = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76aaec35-3548-4397-ab59-9a40a33c5c42",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f69f0c59080a4f5aa0a0f67b06e8c3a7"
          ]
        },
        "id": "76aaec35-3548-4397-ab59-9a40a33c5c42",
        "outputId": "482e3264-566a-4df6-ae85-55daf3294acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading NER model (joon09/kor-naver-ner-name-v2.1)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ NER model loaded!\n",
            "üîÑ Loading VLM model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f69f0c59080a4f5aa0a0f67b06e8c3a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ VLM model loaded!\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# üì¶ NER Î™®Îç∏ Î°úÎìú\n",
        "# ===============================\n",
        "print(\"üîÑ Loading NER model (joon09/kor-naver-ner-name-v2.1)...\")\n",
        "ner_model_name = \"joon09/kor-naver-ner-name-v2.1\"\n",
        "config = AutoConfig.from_pretrained(ner_model_name)\n",
        "id2label = config.id2label\n",
        "tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
        "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
        "print(\"‚úÖ NER model loaded!\")\n",
        "\n",
        "# ===============================\n",
        "# üî∫ VLM MODEL LOAD\n",
        "# ===============================\n",
        "print(\"üîÑ Loading VLM model...\")\n",
        "model = Qwen3VLForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "print(\"‚úÖ VLM model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8eb4bf-185f-42b9-a982-25b7f5b4262f",
      "metadata": {
        "id": "7d8eb4bf-185f-42b9-a982-25b7f5b4262f"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# üß© UTILS\n",
        "# ===============================\n",
        "def natural_key(s: str):\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
        "\n",
        "def normalize_ko(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[„Ñ±-„Öé]\", \"\", s)\n",
        "    s = re.sub(r\"[^0-9a-zÍ∞Ä-Ìû£]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def seq_ratio(a: str, b: str) -> float:\n",
        "    if not a or not b: return 0.0\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def flatten_parsed_text(text: str) -> str:\n",
        "    if not text: return \"\"\n",
        "    s = re.sub(r\"[\\n\\t]+\", \" \", text)\n",
        "    s = re.sub(r\"[„Ñ±-„Öé]\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def flatten_parsed_text2(text: str) -> str:\n",
        "    if not text: return \"\"\n",
        "    s = re.sub(r\"[\\n\\t]+\", \" \", text)\n",
        "    return s.strip()\n",
        "\n",
        "def build_words_list(ocr_lines, y_thresh=15):\n",
        "    raw_words = []\n",
        "    for line in ocr_lines:\n",
        "        for w in line.get(\"Words\", []):\n",
        "            txt = w.get(\"WordText\", \"\")\n",
        "            bbox = w.get(\"bbox\", None)\n",
        "            if not txt or not bbox:\n",
        "                continue\n",
        "            x1, y1 = bbox[0]\n",
        "            x3, y3 = bbox[2]\n",
        "            y_center = (y1 + y3) / 2\n",
        "            raw_words.append({\n",
        "                \"text\": txt, \"text_norm\": normalize_ko(txt),\n",
        "                \"bbox\": bbox, \"y_center\": y_center, \"x_left\": x1\n",
        "            })\n",
        "    if not raw_words:\n",
        "        return []\n",
        "    raw_words.sort(key=lambda x: x[\"y_center\"])\n",
        "\n",
        "    lines, current_line = [], [raw_words[0]]\n",
        "    prev_y = raw_words[0][\"y_center\"]\n",
        "    for w in raw_words[1:]:\n",
        "        if abs(w[\"y_center\"] - prev_y) < y_thresh:\n",
        "            current_line.append(w)\n",
        "        else:\n",
        "            lines.append(current_line)\n",
        "            current_line = [w]\n",
        "        prev_y = w[\"y_center\"]\n",
        "    lines.append(current_line)\n",
        "\n",
        "    words = []\n",
        "    for line in lines:\n",
        "        sorted_line = sorted(line, key=lambda x: x[\"x_left\"])\n",
        "        for w in sorted_line:\n",
        "            words.append({\n",
        "                \"text\": w[\"text\"],\n",
        "                \"text_norm\": w[\"text_norm\"],\n",
        "                \"bbox\": w[\"bbox\"]\n",
        "            })\n",
        "    return words\n",
        "\n",
        "def merge_bboxes(bboxes):\n",
        "    xs1 = [b[0][0] for b in bboxes]\n",
        "    ys1 = [b[0][1] for b in bboxes]\n",
        "    xs2 = [b[2][0] for b in bboxes]\n",
        "    ys2 = [b[2][1] for b in bboxes]\n",
        "    return [[min(xs1), min(ys1)], [max(xs2), min(ys1)], [max(xs2), max(ys2)], [min(xs1), max(ys2)]]\n",
        "\n",
        "def bbox_iou(b1, b2):\n",
        "    p1, p2 = Polygon(b1), Polygon(b2)\n",
        "    inter = p1.intersection(p2).area\n",
        "    union = p1.union(p2).area\n",
        "    return inter / union if union > 0 else 0.0\n",
        "\n",
        "# Ïù¥Î¶Ñ/Ï£ºÏÜå Îß§Ïπ≠ Ìï®Ïàò Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ\n",
        "# ===============================\n",
        "# Ïù¥Î¶Ñ Îß§Ïπ≠\n",
        "# ===============================\n",
        "def find_name_matches_by_length(words, target, masked_bboxes=None,\n",
        "                                same_thresh=0.9, sim_thresh=0.6):\n",
        "    if not target or len(words) == 0:\n",
        "        return []\n",
        "    target_raw = target.strip()\n",
        "    target_len_raw = len(target_raw)\n",
        "    target_norm = normalize_ko(target)\n",
        "    best_score, best_bbox, best_words = 0.0, None, []\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        seg_text, seg_bboxes, seg_words = \"\", [], []\n",
        "        for j in range(i, len(words)):\n",
        "            seg_text += words[j][\"text_norm\"]\n",
        "            seg_bboxes.append(words[j][\"bbox\"])\n",
        "            seg_words.append(words[j][\"text\"])\n",
        "            if len(seg_text) > target_len_raw + 1: break\n",
        "            if len(seg_text) < target_len_raw: continue\n",
        "            score = SequenceMatcher(None, target_norm, seg_text).ratio()\n",
        "            if score > best_score:\n",
        "                best_score, best_bbox, best_words = score, merge_bboxes(seg_bboxes), seg_words.copy()\n",
        "\n",
        "    if target_len_raw <= 2 and (not best_bbox or best_score < 0.9):\n",
        "        #print(f\"‚ö†Ô∏è ÏßßÏùÄ Ïù¥Î¶Ñ Ï†ÄÏú†ÏÇ¨ÎèÑ Ïä§ÌÇµ: {target} (sim={best_score:.2f})\")\n",
        "        return []\n",
        "    if not best_bbox or best_score < sim_thresh:\n",
        "        #print(f\"‚ö†Ô∏è Ïù¥Î¶Ñ ÎØ∏Îß§Ïπ≠ (sim={best_score:.2f}): {target}\")\n",
        "        return []\n",
        "    if masked_bboxes:\n",
        "        for mb in masked_bboxes:\n",
        "            if bbox_iou(mb, best_bbox) > same_thresh:\n",
        "                print(f\"‚è© ÎèôÏùº ÏúÑÏπò Ïù¥Î¶Ñ Ïä§ÌÇµ: {target}\")\n",
        "                return []\n",
        "    #print(f\"‚úÖ Ïù¥Î¶Ñ Îß§Ïπ≠: {target} ‚Üî OCR Îã®Ïñ¥ {best_words} (sim={best_score:.2f})\")\n",
        "    return [best_bbox]\n",
        "\n",
        "# ===============================\n",
        "# üß© NER Ïù¥Î¶Ñ Î≥ëÌï© Ìï®Ïàò\n",
        "# ===============================\n",
        "def collect_full_names(ner_results):\n",
        "    \"\"\"\n",
        "    NER Í≤∞Í≥ºÏóêÏÑú B-PER, I-PER Ïó∞ÏÜç Íµ¨Í∞ÑÏùÑ ÌïòÎÇòÏùò Ïù¥Î¶ÑÏúºÎ°ú Î≥ëÌï©\n",
        "    ex) [{'entity':'B-PER','word':'ÏÜ°'}, {'entity':'I-PER','word':'##ÏÑ†'}, {'entity':'I-PER','word':'##Ïû¨'}]\n",
        "        ‚Üí ['ÏÜ°ÏÑ†Ïû¨']\n",
        "    \"\"\"\n",
        "    names = []\n",
        "    cur = \"\"\n",
        "    for ent in ner_results:\n",
        "        if \"PER\" not in ent[\"entity\"]:\n",
        "            if cur:\n",
        "                names.append(cur)\n",
        "                cur = \"\"\n",
        "            continue\n",
        "        w = ent[\"word\"].replace(\"##\", \"\")\n",
        "        if ent[\"entity\"].startswith(\"B-\"):\n",
        "            if cur:\n",
        "                names.append(cur)\n",
        "            cur = w\n",
        "        elif ent[\"entity\"].startswith(\"I-\"):\n",
        "            cur += w\n",
        "    if cur:\n",
        "        names.append(cur)\n",
        "    return names\n",
        "\n",
        "# ===============================\n",
        "# Ï£ºÏÜå Îß§Ïπ≠\n",
        "# ===============================\n",
        "def find_address_span_bboxes_strict_skip(words, target, masked_bboxes,\n",
        "                                         same_thresh=0.9, sim_thresh=0.6,\n",
        "                                         window=6, max_area_ratio=0.4):\n",
        "    target_norm = normalize_ko(target)\n",
        "    if not target_norm or len(words) == 0:\n",
        "        return []\n",
        "    best_score, best_bbox = 0.0, None\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i + 1, min(i + window, len(words)) + 1):\n",
        "            seg_text = \"\".join([w[\"text_norm\"] for w in words[i:j]])\n",
        "            score = SequenceMatcher(None, target_norm, seg_text).ratio()\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_bbox = merge_bboxes([w[\"bbox\"] for w in words[i:j]])\n",
        "    if best_score < sim_thresh or best_bbox is None:\n",
        "        #print(f\"‚ö†Ô∏è Ï£ºÏÜå ÎØ∏Îß§Ïπ≠ (sim={best_score:.2f}): {target}\")\n",
        "        return []\n",
        "    max_x = max(w[\"bbox\"][2][0] for w in words)\n",
        "    max_y = max(w[\"bbox\"][2][1] for w in words)\n",
        "    total_area = max_x * max_y\n",
        "    area = abs((best_bbox[2][0]-best_bbox[0][0])*(best_bbox[2][1]-best_bbox[0][1]))\n",
        "    if area > total_area * max_area_ratio:\n",
        "        print(f\"‚ö†Ô∏è Skipped huge bbox ({area/total_area:.1%} area): {target}\")\n",
        "        return []\n",
        "    for mb in masked_bboxes:\n",
        "        if bbox_iou(mb, best_bbox) > same_thresh:\n",
        "            #print(f\"‚è© ÎèôÏùº ÏúÑÏπò Ï£ºÏÜå Ïä§ÌÇµ: {target}\")\n",
        "            return []\n",
        "    #print(f\"‚úÖ Ï£ºÏÜå Îß§Ïπ≠ (sim={best_score:.2f}): {target}\")\n",
        "    return [best_bbox]\n",
        "# ===============================\n",
        "# ‚ö° Îπ†Î•∏ OpenCV Ï†ÑÏ≤òÎ¶¨\n",
        "# ===============================\n",
        "def preprocess_image_cv2(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.convertScaleAbs(img, alpha=1.3, beta=0)\n",
        "    kernel = np.array([[0, -1, 0],\n",
        "                       [-1, 5, -1],\n",
        "                       [0, -1, 0]])\n",
        "    img = cv2.filter2D(img, -1, kernel)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eee3dd6-2a47-41bc-aee9-062b8f584f17",
      "metadata": {
        "id": "7eee3dd6-2a47-41bc-aee9-062b8f584f17"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# üìÇ ÌååÏùº ÌéòÏñ¥ÎßÅ\n",
        "# ===============================\n",
        "img_files = sorted([f for f in os.listdir(IMG_DIR) if f.lower().endswith(\".png\")], key=natural_key)\n",
        "json_files = sorted([f for f in os.listdir(JSON_DIR) if f.lower().endswith(\".json\")], key=natural_key)\n",
        "img_map = {os.path.splitext(f)[0]: f for f in img_files}\n",
        "json_map = {os.path.splitext(f)[0]: f for f in json_files}\n",
        "common = sorted(set(img_map.keys()) & set(json_map.keys()), key=natural_key)\n",
        "file_pairs = [(img_map[k], json_map[k]) for k in common]\n",
        "print(f\"‚úÖ Ï¥ù {len(file_pairs)}Í∞ú ÌååÏùº ÌéòÏñ¥ÎßÅ ÏôÑÎ£å\")\n",
        "\n",
        "# ===============================\n",
        "# üß† Î≥ëÎ†¨ Î¨∏ÏÑú Ï≤òÎ¶¨ Ìï®Ïàò\n",
        "# ===============================\n",
        "def process_document(pair):\n",
        "    img_file, json_file = pair\n",
        "    img_path = os.path.join(IMG_DIR, img_file)\n",
        "    json_path = os.path.join(JSON_DIR, json_file)\n",
        "    masked_path = os.path.join(OUT_DIR, f\"masked_{os.path.splitext(img_file)[0]}.png\")\n",
        "\n",
        "    image = preprocess_image_cv2(img_path)\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        ocr = json.load(f)\n",
        "    parsed_text_raw = ocr[\"ParsedResults\"][0].get(\"ParsedText\", \"\")\n",
        "    ocr_lines = ocr[\"ParsedResults\"][0][\"TextOverlay\"][\"Lines\"]\n",
        "    words = build_words_list(ocr_lines)\n",
        "\n",
        "    # numpy Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ Î∞∞Ïó¥\n",
        "    np_image = np.array(image)\n",
        "    mask = np.zeros_like(np_image[:, :, 0], dtype=np.uint8)\n",
        "    masked_bboxes = []\n",
        "\n",
        "    # === 1Ô∏è‚É£ VLM Ï∂îÎ°† ===\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": (\n",
        "                \"Carefully examine this Korean document image **from top-left to bottom-right**, \"\n",
        "                \"reading it section by section just as a person would read a form.\\n\\n\"\n",
        "                \"Guidelines:\\n\"\n",
        "                \"- When reading, look for **labels** like 'ÏÑ±Î™Ö', 'ÏòàÍ∏àÏ£ºÎ™Ö', 'Ïã†Ï≤≠Ïù∏', 'ÎåÄÌëúÏûê' Îì±, \"\n",
        "                \"and extract the corresponding **Name** to the right or below.\\n\"\n",
        "                \"- For 'Ï£ºÏÜå', 'ÏÜåÏû¨ÏßÄ', 'Î≥ÄÍ≤ΩÏ†Ñ', 'Î≥ÄÍ≤ΩÌõÑ', 'ÌòÑÏ£ºÏÜå' Îì±, extract the **Address** nearby.\\n\"\n",
        "                \"- Names are short Korean words (2‚Äì4 syllables).\\n\"\n",
        "                \"- Addresses contain tokens like 'Ïãú', 'Íµ∞', 'Íµ¨', 'Ïùç', 'Î©¥', 'Îèô', 'Î¶¨', 'Í∏∏', 'Î°ú'.\\n\\n\"\n",
        "                \"Exclude labels and return valid JSON only: \"\n",
        "                '{\"names\": [...], \"addresses\": [...]}.\\n\\n'\n",
        "            )},\n",
        "        ],\n",
        "    }]\n",
        "    chat = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(text=[chat], images=[image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            gen_ids = model.generate(**inputs, max_new_tokens=512, temperature=1.2, top_p=0.98)\n",
        "        output_text = processor.batch_decode(gen_ids[:, inputs[\"input_ids\"].shape[1]:],\n",
        "                                             skip_special_tokens=True)[0]\n",
        "        m = re.search(r\"\\{[\\s\\S]*\\}\", output_text)\n",
        "        if not m:\n",
        "            return\n",
        "        parsed = json.loads(m.group(0))\n",
        "        names, addresses = parsed.get(\"names\", []), parsed.get(\"addresses\", [])\n",
        "    except Exception:\n",
        "        return\n",
        "\n",
        "    # === 2Ô∏è‚É£ VLM ÎßàÏä§ÌÇπ ===\n",
        "    for name in names:\n",
        "        boxes = find_name_matches_by_length(words, name, masked_bboxes)\n",
        "        for b in boxes:\n",
        "            x1, y1, x2, y2 = map(int, [b[0][0], b[0][1], b[2][0], b[2][1]])\n",
        "            mask[y1:y2, x1:x2] = 255\n",
        "            masked_bboxes.append(b)\n",
        "    for addr in addresses:\n",
        "        boxes = find_address_span_bboxes_strict_skip(words, addr, masked_bboxes)\n",
        "        for b in boxes:\n",
        "            x1, y1, x2, y2 = map(int, [b[0][0], b[0][1], b[2][0], b[2][1]])\n",
        "            mask[y1:y2, x1:x2] = 255\n",
        "            masked_bboxes.append(b)\n",
        "\n",
        "    # === 3Ô∏è‚É£~4Ô∏è‚É£ NER Î≥¥Ï†ï (Í∏∞Ï°¥ Î°úÏßÅ Í∑∏ÎåÄÎ°ú)\n",
        "    parsed_text_clean = flatten_parsed_text2(parsed_text_raw)\n",
        "    words_split = parsed_text_clean.split()\n",
        "    address_words = [normalize_ko(addr) for addr in addresses]\n",
        "\n",
        "    for w in words_split:\n",
        "        norm_w = normalize_ko(w)\n",
        "        if re.search(r\"[„Ñ±-„Öé]\", w): continue\n",
        "        if any(norm_w in addr for addr in address_words): continue\n",
        "\n",
        "        results = ner_pipeline(w)\n",
        "        has_per = any(\"PER\" in ent[\"entity\"] for ent in results)\n",
        "        has_loc = any(\"LOC\" in ent[\"entity\"] for ent in results)\n",
        "        if has_per or has_loc:\n",
        "            for word_obj in words:\n",
        "                if normalize_ko(word_obj[\"text\"]) == norm_w:\n",
        "                    x1, y1, x2, y2 = map(int, [word_obj[\"bbox\"][0][0], word_obj[\"bbox\"][0][1],\n",
        "                                               word_obj[\"bbox\"][2][0], word_obj[\"bbox\"][2][1]])\n",
        "                    mask[y1:y2, x1:x2] = 255\n",
        "                    masked_bboxes.append(word_obj[\"bbox\"])\n",
        "\n",
        "    # === Ï†ÄÏû• ===\n",
        "    np_image[mask == 255] = 0\n",
        "    Image.fromarray(np_image).save(masked_path)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üßµ Î≥ëÎ†¨ Ïã§Ìñâ\n",
        "# ===============================\n",
        "max_workers = min(4, os.cpu_count())\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    list(tqdm(executor.map(process_document, file_pairs), total=len(file_pairs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe3567f-70c7-4cd5-88f1-bb81033f0de3",
      "metadata": {
        "id": "8fe3567f-70c7-4cd5-88f1-bb81033f0de3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82845d49-094c-4853-bffe-92b862c86316",
      "metadata": {
        "id": "82845d49-094c-4853-bffe-92b862c86316"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}